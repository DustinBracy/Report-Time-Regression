---
title: "Team DAY Project"
author: "Dustin Bracy, Adam Ruthford, Yang Zhang"
date: "1/23/2020"
output: word_document
---

```{r LoadDataLibraries}    
library(tidyverse)
library(data.table) #for %like% function
library(xml2)
library(purrr)
library(lubridate)
library(tictoc)
library(kableExtra)
library(car)

tic('total runtime')
load("./data/reports.RData")
load("./data/customGroups.RData")
load("./data/currencyCode.RData")

```

## Objective 2

	We ran a two-way ANOVA on the data to satisfy the second objective. The business requirements drove the selection of the explanatory variables. The business wants to know what time of day and on which server new reports should be added based upon current report run times as expressed by the ReportDeliveryTime response variable.

###Explanatory and Response variable creation	

	The two explanatory variables require some explanation. The “server” variable required us to remove the data associated with testing and development servers. The testing and development report run times have very different characteristics than production data. Thus, all data associated with servers “SQLODR1” and “SQLODR5” were removed.

	“HourBinned” variable indicates the time of day the report ran. Several things should be noted about it. A maintenance window exists every night between the hours of midnight and six AM. Only a few reports run during this maintenance window. Likewise, relatively few reports run during the late evening hours between eight PM and midnight. We binned these two time periods separately. The remaining time during the days was binned in two-hour blocks. The summary tables below give counts and means of the various bins.

```{r FilterAndBin}
reports$HourBinned <- case_when(reports$HourOfDay %in% c("06","07") ~ "0607",
                                    reports$HourOfDay %in% c("08","09") ~ "0809",
                                    reports$HourOfDay %in% c("10","11")~ "1011",
                                    reports$HourOfDay %in% c("12","13")~ "1213",
                                    reports$HourOfDay %in% c("14","15")~ "1415",
                                    reports$HourOfDay %in% c("16","17")~ "1617",
                                    reports$HourOfDay %in% c("18","19")~ "1819",
                                    reports$HourOfDay %in% c("20","21","22","23")~ "2023",
                                    reports$HourOfDay %in% c("00","01","02","03","04","05")~ "0005",
                                    TRUE~"OTHER")

mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/length(x))
  names(result)<-c("N","Mean","SD","SE")
  return(result)
}

servestats <-aggregate(ReportDeliveryTime~Server,data=reports,mysummary)
servestats.df <- data.frame(servestats[,1:1],servestats[,-(1)])
servestats.df
```

The table above shows that server “SQLODR3” runs fewer reports than the other two servers and has differences in mean report run time and standard deviation.

```{r continuesummary}
hourstats <-aggregate(ReportDeliveryTime~HourBinned,data=reports,mysummary)
hourstats.df <- data.frame(hourstats[,1:1],hourstats[,-(1)])
hourstats.df

sumstats<-aggregate(ReportDeliveryTime~HourBinned:Server,data=reports,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
#head(sumstats, 30)
```

Looking at the data above for the BinnedHour variable the late evening hours and the overnight maintenance window bins display very different standard deviations than the rest of the bins. Also, of note the sample count size differs for those two bins. Finally reports should not be added to the overnight bin that includes the maintenance window.

```{r numReportPlot}


ggplot(mapping=aes(x=HourBinned,y=N,color=Server,group=Server),data=sumstats)+
  geom_line()+
  geom_point()+ylab("Number of reports run") + xlab("Binned time report ran")+
  ggtitle("Number of reports ran by time of day per server")


```
The graph show that each server tracks the others in terms of reports run at a particular hour. A simple interpretation of the results benefits the business directly. The first analysis of the data included an unlogged response variable. The bins for overnight and late evening (“0005”,”2023”)are excluded from the analysis.

```{r UnloggedResponseResidual}

#Remove early morning late evening

reports.fltr <- reports %>% filter(!HourBinned == "0005" & !HourBinned == "2023")

model.fit.HrBnSrv <- aov(ReportDeliveryTime~Server+HourBinned+HourBinned:Server, data = reports.fltr)
anova.disp <- Anova(model.fit.HrBnSrv,type=3,singular.ok=FALSE)

anova.disp[4]
```

The type 3 ANOVA table indicates that both terms as well as the interaction term are statistically significant with very low p-values.

```{r ANOVAIIINoLog}
myfits<-data.frame(fitted.values=model.fit.HrBnSrv$fitted.values,residuals=model.fit.HrBnSrv$residuals)

#Residual vs Fitted

plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()+ggtitle("Log ReportDeliveryTime Residuals")

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))+
  ggtitle("Log ReportDeliveryTime Q-Q plot")

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=1,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")+
  ggtitle("Log ReportDeliveryTime Histogram")

plot1
plot2
# plot3 Don't run this one takes too long and we are not going to use it anyway
```

The residual plot appears to demonstrate a cone shape with residuals beneath 2000 being noticeably more spread out than those above 3000. The Q-Q plot displays dramatic differences from normality. A log transform of the response variable is necessary.

```{r loggedResponsesummary}
reports.fltr$ReportDeliveryTimeLog <- log(reports.fltr$ReportDeliveryTime)

model.fit.HrBnSrvlg <- aov(ReportDeliveryTimeLog~Server+HourBinned+HourBinned:Server, data = reports.fltr)
anova.fltr.disp <- Anova(model.fit.HrBnSrvlg,type=3,singular.ok=FALSE)

anova.fltr.disp[4]
```

The type III ANOVA table again shows that both the interaction term as well as the individual terms are significant.

```{r Summaryloged}
mysummary<-function(x){
  result<-c(length(x),mean(x),sd(x),sd(x)/length(x))
  names(result)<-c("N","Mean","SD","SE")
  return(result)
}

servestats.fltr <-aggregate(ReportDeliveryTimeLog~Server,data=reports.fltr,mysummary)
servestats.fltr.df <- data.frame(servestats.fltr[,1:1],servestats.fltr[,-(1)])
servestats.fltr.df
```

The mean and standard deviation of the logged ReportDeliveryTime track extremely close now with respect to the server variable.

```{r logbinsummary}
hourstats.fltr <-aggregate(ReportDeliveryTimeLog~HourBinned,data=reports.fltr,mysummary)
hourstats.fltr.df <- data.frame(hourstats.fltr[,1:1],hourstats.fltr[,-(1)])
hourstats.fltr.df

sumstats.log.fltr<-aggregate(ReportDeliveryTimeLog~HourBinned:Server,data=reports.fltr,mysummary)
sumstats.log.fltr<-cbind(sumstats.log.fltr[,1:2],sumstats.log.fltr[,-(1:2)])
#sumstats.log.fltr
```

The HourBinned variable does display some differences in standard deviation. This issue is examined below. Please note that the over night and late evening hours were dropped from the model.

	The difference in standard deviation between the various groups is concerning. On-line research yielded the following article https://link.springer.com/article/10.3758/s13428-017-0918-2?shared-article-renderer#Tab6 . The article indicates that a useful measure of unequal variance is the Variance ratio. This ratio is simply the largest group variance divided by the smallest group variance. Looking at the summary statistics table for hours binned we find that:
Variance Ratio=  (σ_max )^2/(σ_min )^2 =  〖2.84〗^2/〖1.20〗^2 =5.6
The correlation between the sample size and variance is -0.81. The coefficient of sample size variation (Δn) was calculated to be (SD(Group Sample Size))/(mean(Group Sample Size))=0.932
According to the article’s conclusions with the parameters as calculated highlighted below (Table 10 of the article) the Type I error ratio will be liberal. However, most p-values reported by the two-way ANOVA are very small. The effects of unequal sample size and unequal variance within the sample groups should only affect relatively few term pairings. Assume p-values can be as much as four times larger than the given value. 
Variance ratio	Pairing	Coefficient of sample size variation	Type I error rate
1.6, 1.7, 1.8	+ or −1	0.50	Liberal
2	−1	0.33; 0.50	Liberal
3	1	0.50	Conservative
 	−.50	0.50	Liberal
 	−1	0.33; 0.50	Liberal
5 & 9	1	0.50	Conservative
 	−.50	0.33; 0.50	Liberal
 	−1	0.16; 0.33; 0.50	Liberal

The following graph shows that the mean log of the delivery time varies by the time of day and has a different standard deviation during times of high loads. 

```{r plotbySrvHour}

ggplot(mapping=aes(x=HourBinned,y=Mean,color=Server,group=Server),data=sumstats.log.fltr)+
  geom_line()+
  geom_point()+
  geom_errorbar(aes(ymin=Mean-SD,ymax=Mean+SD),width=.1)+
  ylab("Mean logged response of reports run") + xlab("Binned time report ran")+
  ggtitle("Mean logged delivery time of reports ran by time of day per server")

```

The following plots were created to further check for normality

```{r loggedResponseResidual}
myfits<-data.frame(fitted.values=model.fit.HrBnSrvlg$fitted.values,residuals=model.fit.HrBnSrvlg$residuals)

#Residual vs Fitted

plot1.fltr.log<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()+ggtitle("Log ReportDeliveryTime Residuals Filtered")

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2.fltr.log<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))+
  ggtitle("Log ReportDeliveryTime Q-Q plot Filtered")

#Histogram of residuals
plot3.fltr.log<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=1,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")+
  ggtitle("Log ReportDeliveryTime Histogram Filtered")

plot1.fltr.log
plot2.fltr.log
plot3.fltr.log

```

Residuals, Q-Q plot, and the Histogram all support the assumption of normality.
Finally, a pairwise comparison based upon server and HourBinned was run. Remember to adjust p-value by a factor of four due to the unequal variance and sample sizes.


```{r ContrastsHours}
TukSrv <- TukeyHSD(model.fit.HrBnSrvlg,"Server",conf.level=.95)

TukHrB <- TukeyHSD(model.fit.HrBnSrvlg,"HourBinned",conf.level=.95)

TukSrvHrB <- TukeyHSD(model.fit.HrBnSrvlg,"Server:HourBinned",conf.level=.95)

TukSrv[1]
TukHrB[1]
```

Conclusion and discussion
	The business asked that we determine on what server and at what time of day additional reports should run. Due to the residual and Q-Q plot of the untransformed data it was found to be necessary to perform a log transform of the response data. The data responded well to the transformation displaying all the characteristics of normalized data.
Using the table “Mean logged response of reports ran by time of day per server” above we can make the following recommendations.
•	The number of reports run on server “SQLODR03” is about 20 % less than the other two servers. However, at most it has only 18% faster mean report delivery time(95% CI[5.3% - 32%]) this is between server “SQLODR03” and “SQLODR6” for the 10 AM through noon time period
•	The time period 6 AM to 8 AM has the highest mean report delivery time, followed by 8 AM to 10 AM and 6 PM to 8 pm. Every effort should be made to not schedule new reports during these time frames.
•	The time period noon to 2 PM has the lowest mean report delivery time, thus if possible new reports should be added there. The relative loading of the servers is as follows. Server “SQLODR02” has a 48% greater mean report delivery time (95% CI[31%, 68%]) than server”SQLODR06” for that time frame. While server “SQLODR3” has a 27% greater mean report delivery time (95% CI[11% , 44%]) than server ”SQLODR06”. Server ”SQLODR06” is the lightest loaded server as measured by mean report delivery time during the noon to 2PM time frame.
•	The very late evening hours 8PM to midnight remain an option for scheduling reports. Care must be taken that any reports scheduled during that time do not run past midnight into the maintenance window.

