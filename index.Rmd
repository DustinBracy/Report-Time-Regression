---
title: "Team DAY Project"
author: "Dustin Bracy, Adam Ruthford, Yang Zhang"
date: "1/23/2020"
output:
  word_document: 
      reference_docx: "wordStyleRef.docx"
  word: default
editor_options:
  chunk_output_type: console
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(data.table) #for %like% function
library(xml2)
library(purrr)
library(lubridate)
library(tictoc)
library(kableExtra)

# import report data:
load("./data/reports.RData")

# wordStyleRef style changes:
## Set margins to .5"
## Heading 1 creates page break before line
## Heading 5 centers text (#####) 

```

##### ![name](./img/reportLogo.jpg)


# Introduction
We would like to answer the following questions about our data:

1. Can we accurately predict the runtime of our reports?
2. What is the expected average delivery time of a one off report?
3. What are the best & worst times of day, week, or month to run a report?
4. Is there a difference in performance between report servers?
5. What are the biggest impacts on performance of reports?

## Data Description
We have collected report data spanning January 2019 through January 2020. The available parameter list is very large, having over 500 parameters used.  In order to narrow the scope of our project, we focused on completed reports only, and determined the parameters show below would be most useful in predicting report run time. A description of utilized fields is avaiable below:

## Report Metrics
* Priority - controlled report parameter
* QueuedDateTime - when report entered queue
* RenderStartDateTime - when server began work on queued report(s) in queueID batch
* ReportStartDateTime - when server began work on individual report
* ReportEndDateTime - when server completed work on individual report
* RenderEndDateTime - when server completed work on queued report(s) in queueID batch
* ReportBytes - size in bytes of completed report
* RptFormat - format of the report delivered
* SchedFreq - Scheduled Report Frequency
* Server - Server on which the report was built

## Engineered Features (all time is in seconds)
* AgentCount - number of agents selected
* DayOfMonth - Day of the month report ran
* DayOfWeek - Day of the week report ran
* GroupCount - number of clients selected
* HourOfDay - Hour of the day report ran
* LagTime - Time report waited for batch to finish (RenderEndDateTime less ReportEndDateTime)
* QueueTime - Time report entered queue (RenderStartDateTime less QueuedDateTime)
* ReportBuildTime - Time spent to build the report (ReportEndDateTime less ReportStartDateTime)
* ReportDeliveryTime - Time user spent waiting for report (RenderEndDateTime less QueuedDateTime)

# Exploratory Data Analysis

```{r EDA}

# Yang
summary(reports)


```

# Appendix

## Additional Report Field Descriptions
* Agents - Individual AgentIDs
* CurrencyCode - Collection of different lines of business
* GroupCustom - Collection of different client types
* GroupFI - Fully Insured Client
* GroupSF - Self Funded Client
* ReportID - Uniquely identifies a report
* QueueID - Uniquely identifies a queue of one or more reports
* FormID - Uniquely identifies collection of report  parameters

## Report Category Crosswalk:

**Eligibility**
"Eligibility","Accumulator","EventHistory","COBRA","RedCard","ComPsych"  

**Miscellaneous**
"Miscellaneous" 

**Reinsurance**
"Reinsurance","Aggregate Report" 

**Professional Liability**
"PL" 

**Actuarial**
"Actuarial","ISBReports","Renewal", *except when rptCategoryDesc = 'Renewal' and reportGroup = 'CommissionsTFB' use 'Commissions'*

**Finance Commission**
"Commissions","CRMOnlineFinance","CommCnt","CommCompare"

**Claims**
"ClaimsAudit","CMSRepting","Repository","GilsbarPPO","Claims"

**Medical Management**
"Wellness Repository","MedCom","TVC","MedInsight"

**Metrics**
"iTrac","TMS","Customer Service","ODR Admin","PortalStats","AutoAdjudication","Genelco","Supervisor"

**Extract**
"DataExtracts","Large Claims Reports"

**Finance Billing**
"Billing", "Coverage","Refunds","Credit Card Process","Deposit","CheckRecon","Check Register","FinancePremium", "Premium"

## ETL of original data

```{r Appendix.ETL pipeline, eval=FALSE}

tic('total runtime')

# Note I have relabeled this originalReports
load("./data/originalReports.RData")
load("./data/customGroups.RData")
load("./data/currencyCode.RData")

# Remove imported reports:
reports <- reports %>% filter(Server != 'IMPSERVICE')

# Drop report with bad data:
reports <- reports %>% filter(!is.na(RunTimeSEC))
tic('clean XML')

# Clean up the XML:
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseString, '>>', '>')
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseStringClean, '&', '-and-')
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseStringClean, 'Transaction Entry Date Thru', 'TransactionEntryDateThru')
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseStringClean,'(<AgentCode>.*</AgentCode>)<ODRParameters>','<NewDataSet><ODRParameters>\\1')
toc()

tic('XML to Cols')
# Parse XML values into Columns: (this might take a minute or two each)
reports$GroupCustom <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<GroupCustom>', xml_find_all(read_xml(x), './/GroupCustom') %>% xml_text(), NA))
reports$CurrencyCode <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<CurrencyCode>', xml_find_all(read_xml(x), './/CurrencyCode') %>% xml_text() %>% toupper(), NA))
reports$groupFI <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<GroupFI>', xml_find_all(read_xml(x), './/GroupFI') %>% xml_text(), NA))
reports$groupSF <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<GroupSF>', xml_find_all(read_xml(x), './/GroupSF') %>% xml_text(), NA))
reports$Agents <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<AgentCode>', xml_find_all(read_xml(x), './/AgentCode') %>% xml_text(), NA))
toc()

# Count commas to determine number of Agents
reports$AgentCount <- ifelse(is.na(reports$Agents), 0, str_count(reports$Agents, ',')+1)


# Join GroupCount to GroupCustom:
reports <- left_join(reports, CustomGroups, by=c('GroupCustom' = 'CustomGroupCode'))


# Capture original GroupCount
reports$CustomGroupCount <- reports$GroupCount


# Function to count groups for currency codes:
getCCgroups <- function(x = c()){
  df <- setNames(data.frame(cCode=str_split(x,','), stringsAsFactors = F),'ccode')
  df <- inner_join(df, CurrencyCodes, by=c('ccode'='CurrencyCode'))
  return(sum(df$GroupCount, na.rm = T))
}

# Build CurrencyCode counts: (this will take a few minutes)
tic("CC Function")
reports$CurrencyCodeGroups <- sapply(reports$CurrencyCode, function(x) ifelse(is.na(x), 0, getCCgroups(x)))
toc()

# Merge counts:
reports$GroupCount <- ifelse(!is.na(reports$groupFI), 1, reports$GroupCount)
reports$GroupCount <- ifelse(!is.na(reports$groupSF), 1, reports$GroupCount)
reports$GroupCount <- ifelse(is.na(reports$GroupCount), reports$CurrencyCodeGroups, reports$GroupCount)
reports$GroupCount <- ifelse(reports$CurrencyCodeGroups < reports$GroupCount & reports$CurrencyCodeGroups > 0, reports$CurrencyCodeGroups, reports$GroupCount)


# Update report categories: Adam
 

reports$ReportCategory <- case_when(reports$RptCategoryDesc %in% 
                              c("Eligibility","Accumulator","EventHistory","COBRA",
                                "RedCard","ComPsych") ~ "Eligibility",
                            reports$RptCategoryDesc %in% 
                              c("Miscellaneous") ~ "Miscellaneous",
                            reports$RptCategoryDesc %in% 
                              c("Reinsurance","Aggregate Report")~ "Reinsurance",
                            reports$RptCategoryDesc %in% 
                              c("PL")~ "Professional Liability",
                            reports$RptCategoryDesc %in% c("Renewal") & 
                              reports$ReportGroupDescription == "Commissions TFB" ~ "Finance Commission",
                            reports$RptCategoryDesc %in% 
                              c("Actuarial","ISBReports","Renewal")~ "Actuarial",
                            reports$RptCategoryDesc %in% 
                              c("Commissions","CRMOnlineFinance","CommCnt",
                                "CommCompare")~ "Finance Commission",
                            reports$RptCategoryDesc %in% 
                              c("ClaimsAudit","CMSRepting","Repository","GilsbarPPO","Claims")~ "Claims",
                            reports$RptCategoryDesc %in% 
                              c("Wellness Repository","MedCom","TVC","MedInsight")~ "Medical Management",
                            reports$RptCategoryDesc %in% 
                              c("iTrac","TMS","Customer Service","ODR Admin","PortalStats",
                                "AutoAdjudication","Genelco","Supervisor")~ "Metrics",
                            reports$RptCategoryDesc %in% 
                              c("DataExtracts","Large Claims Reports")~ "Extract",
                            reports$RptCategoryDesc %in% 
                              c("Billing", "Coverage","Refunds","Credit Card Process","Deposit"
                                ,"CheckRecon","Check Register","FinancePremium", "Premium")~ "Finance Billing",
                            TRUE~"OTHER")

# Engineered Features - Adam

reports$ReportBuildTime <- difftime(reports$ReportEndDateTime, reports$ReportStartDateTime, units = "secs")
reports$ReportDeliveryTime <- difftime(reports$RenderEndDateTime, reports$QueuedDateTime, units = "secs")
reports$LagTime <- difftime(reports$RenderEndDateTime, reports$ReportEndDateTime, units = "secs")
reports$QueueTime <-  difftime(reports$RenderStartDateTime, reports$QueuedDateTime, units = "secs")
reports$DayOfMonth  <- format(as_datetime(reports$ReportStartDateTime),"%d")
reports$DayOfWeek <- format(as_datetime(reports$ReportStartDateTime),"%A")
reports$HourOfDay <- format(as_datetime(reports$ReportStartDateTime),"%H")


# NAs at a glance:
MissingValues <- sapply(reports, function(x) sum(is.na(x)))
MissingValues %>% kable("html") %>% kable_styling()

# Stash the original data before we drop columns
reportsOrg <- reports

# Drop columns we don't need:
dropColumns <- c('XMLResponseString','ReportGroupDescription','ReportPath','StatusDesc','NotifiedDateTime','DeletedDateTime','FormDeleted','ProcessId','FormId','QueueId','RunTimeInMinsSecs','RptCategoryId','CurrencyCodeGroups','CustomGroupCount','RptCategoryDesc', 'RuntimeSec', 'NearestStartHour','XMLResponseStringClean','RunTimeSEC','ReportStartDay')
reports <- reports[ , !(names(reports) %in% dropColumns)]

# Convert to factors:
reports$SchedFreq <- as.factor(reports$SchedFreq)
reports$TestProdIndicator <- as.factor(reports$TestProdIndicator)
reports$DelivMthd <- as.factor(reports$DelivMthd)
reports$DayOfMonth <- as.factor(reports$DayOfMonth)
reports$DayOfWeek <- as.factor(reports$DayOfWeek)
reports$NearestStartHour <- as.factor(reports$NearestStartHour)
reports$ReportCategory <- as.factor(reports$ReportCategory)
reports$RptFrmt <- as.factor(reports$RptFrmt)
reports$Priority <- as.factor(reports$Priority)
reports$HourOfDay <- as.factor(reports$HourOfDay)
reports$Server <- as.factor(reports$Server)

# Save data for analysis:
save(reports, file="./data/reports.RData")

toc()
```






## Addressing Objective 1:
	Restatement of Problem and the overall approach to solve it Required

### Model Selection Required
		Type of Selection
			Options: LASSO, RIDGE, ELASTIC NET,
			     Stepwise, Forward, Backward, 
		             	     Manual / Intuition,
			     A mix of all of the above.  	

### Checking Assumptions Required
			Residual Plots
			Influential point analysis (Cook’s D and Leverage)

		Compare Competing Models Optional (Helpful if using 2 model strategy)
			Via:  Training and test set split or CV
                                        Possible Metrics: (ASE, AIC, BIC, adj R2, etc)
	
### Parameter Interpretation
		Interpretation  Required
		Confidence Intervals Required
	
### Final conclusions from the analyses of Objective 1 Required



In addition to overall conclusions, feel free to include additional insights or concerns gleaned from the analysis.  What needs to be done next or how could we do it better next time?  

## Addressing Objective 2
State what route you are going to take 2way ANOVA or Time series and summarize the goal.  Required

### Main Analysis Content Required
	This will depend on the route you take.  I’m leaving it open here to see what you do.

### Conclusion/Discussion Required
		The conclusion should reprise the questions and conclusions of objective 2.

### Appendix Required
	Well commented SAS/R Code Required
 	Graphics and summary tables (Can be placed in the appendix or in the written report itself.)


