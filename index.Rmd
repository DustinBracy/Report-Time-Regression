---
title: "Team DAY Project"
author: "Dustin Bracy, Adam Ruthford, Yang Zhang"
date: "1/23/2020"
output:
  word_document: 
      reference_docx: "wordStyleRef.docx"
  word: default
editor_options:
  chunk_output_type: console
always_allow_html: yes
---

```{r setup, include=FALSE, cache=TRUE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(data.table) #for %like% function
library(xml2)
library(lubridate)
library(tictoc)
library(kableExtra)
library(gridExtra)
library(olsrr) #residual plot analysis
library(GGally)
library(leaps)
library(glmnet)
library(caret)
library(car)
library(limma)
library(lsmeans)

# import report data:
load("./data/reports.RData")

# import helper functions:
source("./R/helperFunctions.R")

# wordStyleRef style changes:
## Set margins to .5"
## Heading 1 creates page break before line
## Heading 2 remove space before
## Heading 5 centers text (#####) 

```

##### ![name](./img/reportLogo.jpg)


# Introduction
Gilsbar offers a comprehensive reporting package to both its internal employees and external clients.  As our user base has grown over the last several years, we have seen an increased demand for reports and consequent load on report servers.  Report run times vary significantly, sometimes reports are returned in minutes, and in the worst case: hours.  In order to mitigate server load and improve report response time, we would like to better understand factors affecting delivery time, as well as to be able to provide our users an estimated delivery time of reports.  We would like to answer the following questions about our data:

1. What is the expected average delivery time of a new report?
2. What are the best & worst times of day, week, or month to run a report?
3. Is there a difference in performance between report servers?
4. What are the biggest impacts on performance of reports?

## Data Description
Gilsbar has provided report data spanning a 12 month period from January 2019 through January 2020. The available parameter list is very large, having over 500 parameters used in the past year.  In order to narrow the scope of the project, we focused on completed reports only, and determined the parameters show below would be most useful in predicting report run time. A description of other, non-utilized fields is avaiable below in the appendix. Also listed there is a crosswalk provided which groups like-type reports into 11 categories. 

## Approach Methodology
We'd like to tackle these questions using two separate models for interpretation.  For our first objective, we use Multiple Linear Regression techniques to predict runtime using several of the report metrics and features.  As our second objective, we will utilize two-way ANOVA techniques to compare ReportDeliveryTime vs Server, Report Category, and Report Format.

# Exploratory Data Analysis
Our EDA resulted in the following observations:
* Submission times are most often 6am, with 9am and 6pm as secondary peak times.
* The first day of Month is peak report day
* Scheuled frequencies occur more often by Weekly, One-time, Monthly respectively.
* Reinsurance reports grossly exceed delivery times of all other reports, PL reports deliver fastest.
* Extract reports take the longest for servers to build, with miscellaneous building quickest.
* Finance/Billing and Eligibility reports outnumber others, and PL reports are run the least.(Note: PL Category is newest, so does not contain a full year worth of data).
* Most reports utilize Delivery Method: Web. 
* Production reports far outnumber test reports.
* 3 Servers processed most reports are SQLODR2, SQLODR3, SQLODR6.
* SQLODR2, SQLODR3, and SQLODR6 are all marked as production, while the others are marked as test.
* Popular Report formats are CSV/PDF followed by TXT/EXCEL.
* Reports are genereated more on WeekDays with most occuring on Monday/Tuesday.
* Priorities only have 0.1, 2, 3, and 10, with 10 as the majority.
* SQLODR3 processes less reports than SQLODR2 or SQLODR6, and also appears to have a longer mean build time (i.e. worse performance) than the other two production servers.

```{r Dustins MLR options, include=FALSE, cache=TRUE}

# Basic fit: ~ 33% r^2 and 6.65 MSE
#fit <- lm(log(ReportBuildTime)~ReportCategory + GroupCount + GroupCount*ReportCategory, data=reports)

# Better fit:
fit <- lm(log(ReportBuildTime)~Server+Priority+GroupCount+ReportCategory+DayOfWeek+DayOfMonth+AgentCount+RptFrmt+DelivMthd+HourOfDay, data=reports)

# Better fit with interactions: (takes quite a few minutes) ~41% r^2 and 5.98s MSE
#fit <- lm(log(ReportBuildTime)~ReportCategory + HourOfDay + GroupCount + DayOfMonth +  HourOfDay*ReportCategory + GroupCount*ReportCategory + HourOfDay*DayOfMonth + GroupCount*DayOfMonth, data=reports)

# ReportId fit: (takes a few minutes)
#fit <- lm(log(ReportBuildTime)~as.factor(ReportId), data=reports) # ~79% r^2 and 2.83s MSE
#fit2 <- lm(log(ReportDeliveryTime)~as.factor(ReportId), data=reports) # ~49% r^2 and 4.5s MSE


```

```{r Fit Diagnostics, cache=TRUE, include=FALSE}

########### Report Build Time Section ########### 
summary(fit)

# Split data into test/train (using 90% to increase plotting speed)
trainIndices = sample(1:dim(reports)[1],round(.90 * dim(reports)[1]))
train = reports[trainIndices,]
test = reports[-trainIndices,]

# Make predictions & calculate MSE
p <- predict(fit, interval="predict",newdata = test)
RMSE <- sqrt(mean((p[,1] - log(test$ReportBuildTime))^2))
# Back transform the RMSE:
exp(RMSE)

# Plot the predictions vs test data:
preds <- data.frame(cbind(log(test$ReportBuildTime), p[,1]))

# raw log data:
plot(log(test$ReportBuildTime), p[,1])

# pretty raw log data:
ggplot(preds, aes(log(test$ReportBuildTime), p[,1])) + geom_point() + labs (y="Predicted Log Seconds", x="Actual Log Seconds")

# back transformed data: (note the outliers)
ggplot(preds, aes(test$ReportBuildTime, exp(p[,1]))) + geom_point() + labs (y="Predicted Seconds", x="Actual Seconds")

# Zoom in to see same scale:
ggplot(preds, aes(test$ReportBuildTime, exp(p[,1]))) + geom_point() + labs (y="Predicted Seconds", x="Actual Seconds") + xlim(0, 1500) + ylim(0, 1500) 

########### Report Delivery Time Section ########### 

#summary(fit2)

# Make predictions & calculate MSE
#p2 <- predict(fit2, interval="predict",newdata = test)
#RMSE <- sqrt(mean((p2[,1] - log(test$ReportDeliveryTime))^2))
# Back transform the RMSE:
#exp(RMSE)

# Plot the predictions vs test data:
#preds2 <- data.frame(cbind(log(test$ReportDeliveryTime), p[,1]))

# raw log data:
#plot(log(test$ReportDeliveryTime), p2[,1])

# pretty raw log data:
#ggplot(preds2, aes(log(test$ReportDeliveryTime), p2[,1])) + geom_point() + labs (y="Predicted Log Seconds", x="Actual Log Seconds")

# back transformed data: (note the outliers)
#ggplot(preds2, aes(test$ReportDeliveryTime, exp(p2[,1]))) + geom_point() + labs (y="Predicted Seconds", x="Actual Seconds")

# Zoom in to see same scale:
#ggplot(preds2, aes(test$ReportDeliveryTime, exp(p2[,1]))) + geom_point() + labs (y="Predicted Seconds", x="Actual Seconds") + xlim(0, 20000) + ylim(0, 20000) 


```

# Objective 1:
Using known parameters of a desired report, can we predict how long it will take to run a particular type of report? 

We have found it best to use all parameters made available to us within the transformed 'reports' dataset to fit the response variable “ReportDeliveryTime.”  Within the report, intuition tells us a few key parameters contribute strongly to the delivery time.  For example: ReportBuildTime, QueueTime, LagTime and ReportBytes, cannot be used because they are all unknown before report completion. 

## The Intuitive Model:
Our EDA analysis revealed that there are several categorical variables that don’t show an apparent linear relationship with ReportDeliveryTime.  Also, we found that our response variable, ReportDeliveryTime, is heavily right skewed and likely needs a log transformation in order to continue analysis.

Given some additional knowledge of the dataset, we learned that only three production servers are useful for prediction (SQLODR2, SQLODR3 and SQLODR6) and that the ReportID (Specific Report) may be related to ReportBuildTime.  To explore this, we converted the ReportID to factors and conducted simple linear regression, giving us an R^2 of 0.79 which is very good, however the ReportID has over 1500 factor levels which make the explanation difficult when introducing additional features, so we continued to explore other features and temporarily ignore ReportID.



(TODO)

Because the amount of data is very large and we don’t find a dependency of ReportID with ReportDeliveryTime, we choose a single type of report “ReportID=93”, which is roughly 1/10 of the overall dataset as a representative subset and use it for later analysis.

With this subset, we filtered out a few more parameters and the left 10 parameters for fittings are: Server, ReportCategory, SchedFreq, HourOfDay, DayOfMonth, ReportCategory,Server, AgentCount, GroupCount,DelivMethod. First we did a fit with all parameters and get a resulted R^2 of 0.36. It was not too bad.



### Model Selection Required
		Type of Selection
			Options: LASSO, RIDGE, ELASTIC NET,
			     Stepwise, Forward, Backward, 
		             	     Manual / Intuition,
			     A mix of all of the above.  	

## Checking Assumptions
Linear Regression analysis has basic assumptions of residual normality, linearity of features, constant variance (equal spread), and independence of observations.

The q-q plot below (and previous histogram) shows that the assumption of normality may be violated so we do need to log transform our response variables.  We confirmed this to be correct by fitting a new model after log transformation were performed. 

After transofrmation we observed the q-q plot to exhbit qualities indicating the data is approximately normal: a mostly straight line following closely to the residual 0 line. 

```{r Obj1 Assumptions}

```






			Residual Plots
			Influential point analysis (Cook’s D and Leverage)

		Compare Competing Models Optional (Helpful if using 2 model strategy)
			Via:  Training and test set split or CV
                                        Possible Metrics: (ASE, AIC, BIC, adj R2, etc)
	
### Parameter Interpretation
		Interpretation  Required
		Confidence Intervals Required
	
## Conclusion
In conclusion, the mulit-linear model with 5 predictors leveraging a logged response gave the best MSE with avoidance of overfitting. We used a train and test split to validate the MSE across many method and reach this conclusion.


```{r , cache=TRUE}
reports_serv_samp <- reports %>% filter(ReportId == "93") 
#reports_serv_new <- within(reports_serv_samp,
#rm(ReportId,Priority,RptFrmt,QueuedDateTime,RenderStartDateTime,ReportStartDateTime,ReportEndDateTime, RenderEndDateTime,GroupCustom,CurrencyCode,groupSF,groupFI,ReportBuildTime,QueueTime,LagTime,Agents,TestProdIndicator,ReportBytes))
reports_serv_new <- within(reports_serv_samp,
rm(ReportId,Priority,RptFrmt,QueuedDateTime,RenderStartDateTime,ReportStartDateTime,ReportEndDateTime, RenderEndDateTime,GroupCustom,CurrencyCode,groupSF,groupFI,ReportBuildTime,QueueTime,LagTime,Agents,TestProdIndicator,ReportBytes,HourBinned,TimeBlock3h,TimeBlock4h,DateFrom,DateThru,queued,NumDays))


#Drop levels of unused server names
table(droplevels(reports_serv_new)$Server)
reports_serv_new$Server <- factor(reports_serv_new$Server)
levels(reports_serv_new$Server)

#reports_serv_new <- reports_serv_new[-4301,]
#reports_serv_new <- reports_serv_new[-5178,]
#reports_serv_new <- reports_serv_new[-14441,]

pairs(reports_serv_new[,-c(2,9)])

all_model<-lm(ReportDeliveryTime~., data=reports_serv_new)
summary(all_model)

#par(mfrow=c(2,2))
#plot(all_model)

all_model_log<-lm(log(ReportDeliveryTime)~., data=reports_serv_new)
summary(all_model_log)

#par(mfrow=c(2,2))
#plot(all_model_log)


```

```{r , cache=TRUE}
reports_serv_newlog <- reports_serv_new
reports_serv_newlog$LogReportDelivTime = log(reports_serv_newlog$ReportDeliveryTime)
reports_serv_newlog <- na.omit(reports_serv_newlog)

reports_serv_newlog <- within(reports_serv_newlog,
rm(ReportDeliveryTime))


log.fwdnv20=regsubsets(LogReportDelivTime~.,data=reports_serv_newlog,method="forward",nvmax=30)
summary(log.fwdnv20)
summary(log.fwdnv20)$adjr2
summary(log.fwdnv20)$rss
summary(log.fwdnv20)$bic


par(mfrow=c(1,3))
bics<-summary(log.fwdnv20)$bic
plot(1:31,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(log.fwdnv20)$adjr2
plot(1:31,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(log.fwdnv20)$rss
plot(1:31,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

  
coef(log.fwdnv20,10)

log.stepnv20=regsubsets(LogReportDelivTime~.,data=reports_serv_newlog,method="seqrep",nvmax=30)
summary(log.stepnv20)
summary(log.stepnv20)$adjr2
summary(log.stepnv20)$rss
summary(log.stepnv20)$bic

par(mfrow=c(1,3))
bics<-summary(log.stepnv20)$bic
plot(1:31,bics,type="l",ylab="BIC",xlab="# of predictors")
index<-which(bics==min(bics))
points(index,bics[index],col="red",pch=10)

adjr2<-summary(log.stepnv20)$adjr2
plot(1:31,adjr2,type="l",ylab="Adjusted R-squared",xlab="# of predictors")
index<-which(adjr2==max(adjr2))
points(index,adjr2[index],col="red",pch=10)

rss<-summary(log.stepnv20)$rss
plot(1:31,rss,type="l",ylab="train RSS",xlab="# of predictors")
index<-which(rss==min(rss))
points(index,rss[index],col="red",pch=10)

coef(log.stepnv20,10)

final.model<-lm(LogReportDelivTime~SchedFreq + HourOfDay + DayOfMonth + ReportCategory + Server,data=reports_serv_newlog)
summary(final.model)


```

```{r , cache=TRUE}
set.seed(1234)
index<-sample(1:dim(reports_serv_newlog)[1],20000,replace=F)
train<-reports_serv_newlog[index,]
test<-reports_serv_newlog[-index,]

trainlog.fwdnv30=regsubsets(LogReportDelivTime~.,data=train,method="forward",nvmax=30)
testASE<-c()
#note my index is to 20 since that what I set it in regsubsets
for (i in 1:31){
  predictions_fwd<-predict.regsubsets(object=trainlog.fwdnv30,newdata=test,id=i) 
  testASE[i]<-mean((test$LogReportDelivTime-predictions_fwd)^2)
}
trainlog.stepnv30=regsubsets(LogReportDelivTime~.,data=train,method="seqrep",nvmax=30)

par(mfrow=c(1,1))
plot(1:31,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,2))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(trainlog.stepnv30)$rss
lines(1:31,rss/10000,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size

RMSE_org_Fwd <- RMSE(exp(predictions_fwd), exp(test$LogReportDelivTime))

testASE<-c()
#note my index is to 20 since that what I set it in regsubsets
for (i in 1:31){
  predictions_step<-predict.regsubsets(object=trainlog.stepnv30,newdata=test,id=i) 
  testASE[i]<-mean((test$LogReportDelivTime-predictions_step)^2)
}
par(mfrow=c(1,1))
plot(1:31,testASE,type="l",xlab="# of predictors",ylab="test vs train ASE",ylim=c(0,2))
index<-which(testASE==min(testASE))
points(index,testASE[index],col="red",pch=10)
rss<-summary(trainlog.stepnv30)$rss
lines(1:31,rss/10000,lty=3,col="blue")  #Dividing by 100 since ASE=RSS/sample size

RMSE_org_Step <- RMSE(exp(predictions_step), exp(test$LogReportDelivTime))

```

```{r , cache=TRUE}
final.model.train<-lm(LogReportDelivTime~SchedFreq + HourOfDay + DayOfMonth + ReportCategory + Server,data=train)
predictions_lm <- final.model.train %>% predict(test) %>% as.vector()

RMSE_org_LM <- RMSE(exp(predictions_lm), exp(test$LogReportDelivTime))

```

```{r , cache=TRUE}
#set.seed(1234)
#index<-sample(1:dim(reports_serv_newlog)[1],20000,replace=F)
#train<-reports_serv_newlog[index,]
#test<-reports_serv_newlog[-index,]

#Formatting data for GLM net
x=model.matrix(LogReportDelivTime~.,train)[,-1]
#y=log(train$ReportDeliveryTime)
y=train$LogReportDelivTime

xtest<-model.matrix(LogReportDelivTime~.,test)[,-1]
#ytest<-log(test$ReportDeliveryTime)
ytest<-test$LogReportDelivTime



grid=10^seq(10,-2, length =100)
lasso.mod=glmnet(x,y,alpha=1, lambda =grid)

cv.out=cv.glmnet(x,y,alpha=1) #alpha=1 performs LASSO
plot(cv.out)
bestlambda<-cv.out$lambda.min  #Optimal penalty parameter.  You can make this call visually.
lasso.pred=predict(lasso.mod ,s=bestlambda ,newx=xtest)

testMSE_LASSO<-mean((ytest-lasso.pred)^2)
testMSE_LASSO


# Fit the final model on the training data
model_lasso <- glmnet(x, y, alpha = 1, lambda = bestlambda)
# Display regression coefficients
coef(model_lasso)

#coef(lasso.mod,s=bestlambda)

# Model performance metrics
data.frame(
  RMSE = RMSE(lasso.pred, ytest),
  Rsquare = R2(lasso.pred, ytest)
)

RMSE_org_LASSO <- RMSE(exp(lasso.pred), exp(ytest))

# RIDGE
cv.out.ridge=cv.glmnet(x,y,alpha=0) #alpha=1 performs RIDGE
plot(cv.out.ridge)

cv.out.ridge$lambda.min

# Fit the final model on the training data
model_ridge <- glmnet(x, y, alpha = 0, lambda = cv.out.ridge$lambda.min)
# Display regression coefficients
coef(model_ridge)

# Make predictions on the test data
predictions <- model_ridge %>% predict(xtest) %>% as.vector()

testMSE_RIDGE<-mean((ytest-predictions)^2)
testMSE_RIDGE

plot(predictions, ytest, xlim = c(0,20), ylim = c(0,20))

# Model performance metrics
data.frame(
  RMSE = RMSE(predictions, ytest),
  Rsquare = R2(predictions, ytest)
)

RMSE_org_RIDGE <- RMSE(exp(predictions), exp(ytest))

#Elastic net
# Build the model using the training set
set.seed(123)
model_ela <- train(
  LogReportDelivTime ~., data = train, method = "glmnet",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
# Best tuning parameter
model_ela$bestTune

# Coefficient of the final model. You need
# to specify the best lambda
coef(model_ela$finalModel, model_ela$bestTune$lambda)

# Make predictions on the test data
predictions_ela <- model_ela %>% predict(test) %>% as.vector()

testMSE_ELA<-mean((ytest-predictions_ela)^2)
testMSE_ELA


# Model performance metrics
data.frame(
  RMSE = RMSE(predictions_ela, ytest),
  Rsquare = R2(predictions_ela, ytest)
)

RMSE_org_ELA <- RMSE(exp(predictions_ela), exp(ytest))

```


In addition to overall conclusions, feel free to include additional insights or concerns gleaned from the analysis.  What needs to be done next or how could we do it better next time?  

# Objective 2: Section

## Addressing Objective 2
State what route you are going to take 2way ANOVA or Time series and summarize the goal.  Required



### Notes on Binning HourOfDay and Filtering by server
The HourOfDay variable is going to be binned so that we can examine the time of day as a categorical variable. This is in an effort to determine the best time of the day to run one of or schedule new reports. Servers 1 and 5 are test and development servers. Thus they do not impact the run times for production data.


## Two way Anova

### Unlogged response variable Summary Stats and ANOVA table


```{r UnloggedResponseSummary}
model.fit.HrBnSrv <- aov(ReportDeliveryTime~Server+HourBinned+HourBinned:Server, data = reports)
Anova(model.fit.HrBnSrv,type=3,singular.ok=FALSE)

servestats <-aggregate(ReportDeliveryTime~Server,data=reports,mysummary)
servestats.df <- data.frame(servestats[,1:1],servestats[,-(1)])
servestats.df

hourstats <-aggregate(ReportDeliveryTime~HourBinned,data=reports,mysummary)
hourstats.df <- data.frame(hourstats[,1:1],hourstats[,-(1)])
hourstats.df

sumstats<-aggregate(ReportDeliveryTime~HourBinned:Server,data=reports,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
head(sumstats, 30)

```

### Discussion
ANOVA table does show that interaction as well as explanatory variables are significant. The summary statistics include big differences in standard deviation.

### Unlogged response variable Residual plots

```{r UnloggedResponseResidual}
myfits<-data.frame(fitted.values=model.fit.HrBnSrv$fitted.values,residuals=model.fit.HrBnSrv$residuals)

#Residual vs Fitted

plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()+ggtitle("Log ReportDeliveryTime Residuals")

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))+
  ggtitle("Log ReportDeliveryTime Q-Q plot")

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=1,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")+
  ggtitle("Log ReportDeliveryTime Histogram")

plot1
plot2
# plot3 Don't run this one takes too long and we are not going to use it anyway
```

### Discussion
The residual plots is unacceptable. The Q-Q plot demonstrates significant issues as well. A log transformation will be used to correct these problems

## Two way Anova
### Logged response variable Summary Stats and ANOVA table


```{r loggedResponsesummary}
reports$ReportDeliveryTimeLog <- log(reports$ReportDeliveryTime)

model.fit.HrBnSrvlg <- aov(ReportDeliveryTimeLog~Server+HourBinned+HourBinned:Server, data = reports)
Anova(model.fit.HrBnSrvlg,type=3,singular.ok=FALSE)

sumstats<-aggregate(ReportDeliveryTimeLog~HourBinned:Server,data=reports,mysummary)
sumstats<-cbind(sumstats[,1:2],sumstats[,-(1:2)])
sumstats

```

### Discussion
ANOVA table does show that is not significant. The summary statistics standard deviation is more consistent but has intersting patterns.

### Logged response variable Residual plots

```{r loggedResponseResidual}
myfits<-data.frame(fitted.values=model.fit.HrBnSrvlg$fitted.values,residuals=model.fit.HrBnSrvlg$residuals)

#Residual vs Fitted

plot1<-ggplot(myfits,aes(x=fitted.values,y=residuals))+ylab("Residuals")+
  xlab("Predicted")+geom_point()+ggtitle("Log ReportDeliveryTime Residuals")

#QQ plot of residuals  #Note the diagonal abline is only good for qqplots of normal data.
plot2<-ggplot(myfits,aes(sample=residuals))+
  stat_qq()+geom_abline(intercept=mean(myfits$residuals), slope = sd(myfits$residuals))+
  ggtitle("Log ReportDeliveryTime Q-Q plot")

#Histogram of residuals
plot3<-ggplot(myfits, aes(x=residuals)) + 
  geom_histogram(aes(y=..density..),binwidth=1,color="black", fill="gray")+
  geom_density(alpha=.1, fill="red")+
  ggtitle("Log ReportDeliveryTime Histogram")

plot1
plot2
plot3

```

### Discussion
The residual plot shows definite signs of improvement. The Q-Q plot demonstrates significant issues as well. The histogram shows normal distribution

### To Do Contrasts

```{r ContrastsHours}
TukeyHSD(model.fit.HrBnSrvlg,"HourBinned",conf.level=.95)

contrast.factor<-~HourBinned
mycontrast<-c("1415-1011","1823-1617")
dat<-reports


final.result<-c()
for( j in 1:length(mycontrast)){
  contrast.factor.names<-gsub(" ", "", unlist(strsplit(as.character(contrast.factor),split = "*", fixed = T))[-1])
  contrast.factor.2 <- vector("list", length(contrast.factor.names))
  for (i in 1:length(contrast.factor.names)) {
    contrast.factor.2[[i]] <- levels(dat[, contrast.factor.names[i]])
  }
  new.factor.levels <- do.call(paste, c(do.call(expand.grid, contrast.factor.2), sep = ""))
  temp.cont<-mycontrast[j]
  contrast2 <- list(comparison = as.vector(do.call(makeContrasts, list(contrasts = temp.cont, levels = new.factor.levels))))
  contrast.result <- summary(contrast(lsmeans(model.fit, contrast.factor), contrast2, by = NULL))
  final.result<-rbind(final.result,contrast.result)
}

#Cleaning up and applying bonferroni correction to the number
#of total comparisons investigated.
final.result$contrast<-mycontrast
final.result$bonf<-length(mycontrast)*final.result$p.value
final.result$bonf[final.result$bonf>1]<-1

final.result

```








### Main Analysis Content Required
	This will depend on the route you take.  I’m leaving it open here to see what you do.

### Conclusion/Discussion Required
		The conclusion should reprise the questions and conclusions of objective 2.


# Appendix: Features

## Report Parameters
* Agents - Individual AgentIDs 
* CurrencyCode - Collection of different lines of business
* GroupCustom - Collection of different client types
* GroupFI - Fully Insured Client
* GroupSF - Self Funded Client
* ReportID - Uniquely identifies a report
* QueueID - Uniquely identifies a queue of one or more reports
* FormID - Uniquely identifies collection of report parameters 


## Report Metrics
* Priority - Controlled report parameter
* QueuedDateTime - When report entered queue
* RenderStartDateTime - When server began work on queued report(s) in queueID batch
* ReportStartDateTime - When server began work on individual report
* ReportEndDateTime - When server completed work on individual report
* RenderEndDateTime - When server completed work on queued report(s) in queueID batch
* ReportBytes - Size in bytes of completed report
* RptFormat - Format of the report delivered
* SchedFreq - Scheduled Report Frequency
* Server - Server on which the report was built


## Engineered Features
* AgentCount - Number of agents selected
* DayOfMonth - Day of the month report ran
* DayOfWeek - Day of the week report ran
* GroupCount - Number of clients selected
* HourOfDay - Hour of the day report ran
* LagTime - Time report waited for batch to finish (RenderEndDateTime less ReportEndDateTime)
* QueueTime - Time report entered queue (RenderStartDateTime less QueuedDateTime)
* ReportBuildTime - Time spent to build the report (ReportEndDateTime less ReportStartDateTime)
* ReportDeliveryTime - Time user spent waiting for report (RenderEndDateTime less QueuedDateTime)


# Appendix: Report Categories
## Report Category crosswalk: 
|Category|Groups|
|---|---|
|Eligibility|"Eligibility","Accumulator","EventHistory","COBRA","RedCard","ComPsych"|  
|Miscellaneous|"Miscellaneous"|
|Reinsurance|"Reinsurance","Aggregate Report"|
|Professional Liability|"PL"|
|Actuarial|"Actuarial","ISBReports","Renewal", *except when rptCategoryDesc = 'Renewal' and reportGroup = 'CommissionsTFB' use 'Commissions'*|
|Finance Commission|"Commissions","CRMOnlineFinance","CommCnt","CommCompare"|
|Claims|"ClaimsAudit","CMSRepting","Repository","GilsbarPPO","Claims"|
|Medical Management|"Wellness Repository","MedCom","TVC","MedInsight"|
|Metrics|"iTrac","TMS","Customer Service","ODR Admin","PortalStats","AutoAdjudication","Genelco","Supervisor"|
|Extract|"DataExtracts","Large Claims Reports"|
|Finance Billing|"Billing", "Coverage","Refunds","Credit Card Process","Deposit","CheckRecon","Check Register","FinancePremium", "Premium"|

# Appendix: Data Preparation
## ETL of original data

```{r Appendix.ETL pipeline, eval=FALSE}

tic('total runtime')

# Note I have relabeled this originalReports
load("./data/originalReports.RData")
load("./data/customGroups.RData")
load("./data/currencyCode.RData")

# Remove imported reports & dev servers:
reports <- reports %>% filter(Server %in% c('SQLODR2','SQLODR3','SQLODR6'))

# Drop report with bad data:
reports <- reports %>% filter(!is.na(RunTimeSEC))

tic('clean XML')

# Clean up the XML:
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseString, '>>', '>')
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseStringClean, '&', '-and-')
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseStringClean, 'Transaction Entry Date Thru', 'TransactionEntryDateThru')
reports$XMLResponseStringClean <- str_replace(reports$XMLResponseStringClean,'(<AgentCode>.*</AgentCode>)<ODRParameters>','<NewDataSet><ODRParameters>\\1')
toc()

tic('XML to Cols')
# Parse XML values into Columns: (this might take a minute or two each)
reports$GroupCustom <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<GroupCustom>', xml_find_all(read_xml(x), './/GroupCustom') %>% xml_text(), NA))
reports$CurrencyCode <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<CurrencyCode>', xml_find_all(read_xml(x), './/CurrencyCode') %>% xml_text() %>% toupper(), NA))
reports$groupFI <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<GroupFI>', xml_find_all(read_xml(x), './/GroupFI') %>% xml_text(), NA))
reports$groupSF <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<GroupSF>', xml_find_all(read_xml(x), './/GroupSF') %>% xml_text(), NA))
reports$Agents <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% '<AgentCode>', xml_find_all(read_xml(x), './/AgentCode') %>% xml_text(), NA))
toc()

# Capture number of days:
tic("Dates")
reports$DateFrom <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% 'DateFrom>\\d+'| x %like% 'DateRangeFrom', str_replace(x, '.*Date(RangeFrom|From)>(\\d+)</.*', '\\2'), NA))
reports$DateThru <- sapply(reports$XMLResponseStringClean, function(x) 
  ifelse(x %like% 'DateThru>\\d+'| x %like% 'DateRangeThru', str_replace(x, '.*Date(RangeThru|Thru)>(\\d+)</.*', '\\2'), NA))

reports$queued <- as_date(reports$QueuedDateTime)
reports$queued <- str_replace_all(reports$queued, "-", "")

reports$DateThru <- ifelse(is.na(reports$DateThru),NA,ifelse((as_date(reports$DateThru) > as_date('20251231') | reports$DateThru == "") & as_date(reports$DateFrom) != as_date(reports$DateThru), reports$queued, reports$DateThru))

reports$NumDays <- ifelse(is.na(reports$DateFrom) | is.na(reports$DateThru) , NA , difftime(as_date(reports$DateThru), as_date(reports$DateFrom) , units = "days"))
reports$NumDays <- ifelse(as_date(reports$DateFrom) == as_date(reports$DateThru), 1, reports$NumDays)
reports$NumDays <- abs(reports$NumDays)

reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>(CURRENT|PRIOR)YEAR<', 365, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>(CURRENT|PRIOR)QUARTER<', 91, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>(CURRENT|PRIOR)MONTH<', 30, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>(CURRENT|PRIOR)WEEK<', 7, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>(CURRENT|PRIOR)DAY<', 1, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>ROLLING3MONTHS<', 91, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>ROLLING6MONTHS<', 182, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>ROLLING12MONTHS<', 365, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>ROLLING15MONTHS<', 456, reports$NumDays)
reports$NumDays <- ifelse(reports$XMLResponseStringClean %like% '>ROLLING24MONTHS<', 730, reports$NumDays)
toc()

# Count commas to determine number of Agents
reports$AgentCount <- ifelse(is.na(reports$Agents), 0, str_count(reports$Agents, ',')+1)


# Join GroupCount to GroupCustom:
reports <- left_join(reports, CustomGroups, by=c('GroupCustom' = 'CustomGroupCode'))


# Capture original GroupCount
reports$CustomGroupCount <- reports$GroupCount


# Build CurrencyCode counts: (this will take a few minutes)
tic("CC Function")
reports$CurrencyCodeGroups <- sapply(reports$CurrencyCode, function(x) ifelse(is.na(x), 0, getCCgroups(x)))
toc()

# Merge GroupCounts:
reports$GroupCount <- ifelse(!is.na(reports$groupFI), 1, reports$GroupCount)
reports$GroupCount <- ifelse(!is.na(reports$groupSF), 1, reports$GroupCount)
reports$GroupCount <- ifelse(reports$XMLResponseStringClean %like% '<ExtGroupNumber>', 1, reports$GroupCount)
reports$GroupCount <- ifelse(reports$XMLResponseStringClean %like% '>ALLGROUPS<', 8353, reports$GroupCount)
reports$GroupCount <- ifelse(is.na(reports$GroupCount), reports$CurrencyCodeGroups, reports$GroupCount)
reports$GroupCount <- ifelse(reports$CurrencyCodeGroups < reports$GroupCount & reports$CurrencyCodeGroups > 0, reports$CurrencyCodeGroups, reports$GroupCount)
reports$GroupCount <- ifelse(is.na(reports$GroupCount),8353,reports$GroupCount)
reports$GroupCount <- ifelse((reports$GroupCount == 0),8353,reports$GroupCount)

# Update report categories: Adam


reports$ReportCategory <- case_when(reports$RptCategoryDesc %in% 
                              c("Eligibility","Accumulator","EventHistory","COBRA",
                                "RedCard","ComPsych") ~ "Eligibility",
                            reports$RptCategoryDesc %in% 
                              c("Miscellaneous") ~ "Miscellaneous",
                            reports$RptCategoryDesc %in% 
                              c("Reinsurance","Aggregate Report")~ "Reinsurance",
                            reports$RptCategoryDesc %in% 
                              c("PL")~ "Professional Liability",
                            reports$RptCategoryDesc %in% c("Renewal") & 
                              reports$ReportGroupDescription == "Commissions TFB" ~ "Finance Commission",
                            reports$RptCategoryDesc %in% 
                              c("Actuarial","ISBReports","Renewal")~ "Actuarial",
                            reports$RptCategoryDesc %in% 
                              c("Commissions","CRMOnlineFinance","CommCnt",
                                "CommCompare")~ "Finance Commission",
                            reports$RptCategoryDesc %in% 
                              c("ClaimsAudit","CMSRepting","Repository","GilsbarPPO","Claims")~ "Claims",
                            reports$RptCategoryDesc %in% 
                              c("Wellness Repository","MedCom","TVC","MedInsight")~ "Medical Management",
                            reports$RptCategoryDesc %in% 
                              c("iTrac","TMS","Customer Service","ODR Admin","PortalStats",
                                "AutoAdjudication","Genelco","Supervisor")~ "Metrics",
                            reports$RptCategoryDesc %in% 
                              c("DataExtracts","Large Claims Reports")~ "Extract",
                            reports$RptCategoryDesc %in% 
                              c("Billing", "Coverage","Refunds","Credit Card Process","Deposit"
                                ,"CheckRecon","Check Register","FinancePremium", "Premium")~ "Finance Billing",
                            TRUE~"OTHER")

# Engineered Features - Adam

reports$ReportBuildTime <- as.numeric(difftime(reports$ReportEndDateTime, reports$ReportStartDateTime, units = "secs"))
reports$ReportDeliveryTime <- as.numeric(difftime(reports$RenderEndDateTime, reports$QueuedDateTime, units = "secs"))
reports$LagTime <- as.numeric(difftime(reports$RenderEndDateTime, reports$ReportEndDateTime, units = "secs"))
reports$QueueTime <-  as.numeric(difftime(reports$RenderStartDateTime, reports$QueuedDateTime, units = "secs"))
reports$DayOfMonth  <- format(as_datetime(reports$ReportStartDateTime),"%d")
reports$DayOfWeek <- format(as_datetime(reports$ReportStartDateTime),"%A")
reports$HourOfDay <- format(as_datetime(reports$ReportStartDateTime),"%H")


# NAs at a glance:
MissingValues <- sapply(reports, function(x) sum(is.na(x)))
MissingValues %>% kable("html") %>% kable_styling()

# Drop extreme outliers, reports which are highly likely to be errors:
reports <- reports %>% filter(ReportDeliveryTime < 300000)

# Remove delivery time < 0
reports <- reports %>% filter(ReportDeliveryTime > 0)

# Convert to factors:
reports$SchedFreq <- as.factor(reports$SchedFreq)
reports$TestProdIndicator <- as.factor(reports$TestProdIndicator)
reports$DelivMthd <- as.factor(reports$DelivMthd)
reports$DayOfMonth <- as.factor(reports$DayOfMonth)
reports$DayOfWeek <- as.factor(reports$DayOfWeek)
reports$ReportCategory <- as.factor(reports$ReportCategory)
reports$RptFrmt <- as.factor(reports$RptFrmt)
reports$Priority <- as.factor(reports$Priority)
reports$HourOfDay <- as.factor(reports$HourOfDay)
reports$Server <- as.factor(reports$Server)

# Add some time binning variables:
reports$HourBinned <- case_when(reports$HourOfDay %in% c("06","07") ~ "0607",
                                    reports$HourOfDay %in% c("08","09") ~ "0809",
                                    reports$HourOfDay %in% c("10","11")~ "1011",
                                    reports$HourOfDay %in% c("12","13")~ "1213",
                                    reports$HourOfDay %in% c("14","15")~ "1415",
                                    reports$HourOfDay %in% c("16","17")~ "1617",
                                    reports$HourOfDay %in% c("18","19","20","21","22","23")~ "1823",
                                    reports$HourOfDay %in% c("00","01","02","03","04","05")~ "0005",
                                    TRUE~"OTHER")

reports$TimeBlock3h <- case_when(reports$HourOfDay %in% 
                              c("00","01","02","03","04","05") ~ "Maintenance",
                            reports$HourOfDay %in% 
                              c("06","07","08") ~ "6am-8am",
                            reports$HourOfDay %in% 
                              c("09","10","11")~ "9am-11am",
                            reports$HourOfDay %in% 
                              c("12","13","14")~ "12pm-2pm",
                            reports$HourOfDay %in% 
                              c("15","16","17")~ "3pm-5pm",
                            reports$HourOfDay %in% 
                              c("18","19","20","21","22","23")~ "Overnight",
                            TRUE~"OTHER")

reports$TimeBlock4h <- case_when(reports$HourOfDay %in% 
                              c("00","01","02","03","04","05") ~ "Maintenance",
                            reports$HourOfDay %in% 
                              c("06","07","08","09") ~ "6am-9am",
                            reports$HourOfDay %in% 
                              c("10","11","12","13")~ "10am-1pm",
                            reports$HourOfDay %in% 
                              c("14","15","16","17")~ "2pm-5pm",
                            reports$HourOfDay %in% 
                              c("18","19","20","21","22","23")~ "Overnight",
                            TRUE~"OTHER")


# Stash the original data before we drop columns
reportsOrg <- reports

# Drop columns we don't need:
dropColumns <- c('XMLResponseString','ReportGroupDescription','ReportPath','StatusDesc','NotifiedDateTime','DeletedDateTime','FormDeleted','ProcessId','FormId','QueueId','RunTimeInMinsSecs','RptCategoryId','CurrencyCodeGroups','CustomGroupCount','RptCategoryDesc', 'RuntimeSec', 'NearestStartHour','XMLResponseStringClean','RunTimeSEC','ReportStartDay')

reports <- reports[ , !(names(reports) %in% dropColumns)]

# Save data for analysis:
save(reports, file="./data/reports.RData")
save(reportsOrg, file="./data/reportsFull.RData")

toc()



```

# Appendix: EDA Plots:

```{r EDA plots, fig.width=7.25, fig.height=4.5, echo=TRUE, cache=TRUE}
# Plot Categorical Variables in character or factor

# Count By Server
e.server <- reports %>% group_by(Server) %>% summarise(Count=n()) %>% ggplot(aes(x=Server, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Server", x="Server",y="Count") + geom_text(aes(x=Server, y=0.01, label= Count), vjust=-2, size=3, colour="black", fontface="bold",angle=360)

# Count By SchedFreq
e.freq <- reports %>% group_by(SchedFreq) %>% summarise(Count=n()) %>% ggplot(aes(x=SchedFreq, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Schedule Frequency", x="Schedule Frequency",y="Count") + geom_text(aes(x=SchedFreq, y=0.01, label= Count), vjust=-2, size=3, colour="black", fontface="bold",angle=360)

# Count By RptFrmt
e.format <- reports %>% group_by(RptFrmt) %>% summarise(Count=n()) %>% ggplot(aes(x=RptFrmt, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Report Format", x="Report Format",y="Count") + geom_text(aes(x=RptFrmt, y=0.01, label= Count), vjust=-2, size=3, colour="black", fontface="bold",angle=360)

# Count By DelivMthd
e.delivery <- reports %>% group_by(DelivMthd) %>% summarise(Count=n()) %>% ggplot(aes(x=DelivMthd, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Delivery Method", x="Delivery Method",y="Count") + geom_text(aes(x=DelivMthd, y=0.01, label= Count), vjust=-2, size=3, colour="black", fontface="bold",angle=360)

# Count By Priority
e.priority <- reports %>% group_by(Priority) %>% summarise(Count=n()) %>% ggplot(aes(x=Priority, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Priority", x="Priority",y="Count") + geom_text(aes(x=Priority, y=0.01, label= Count), vjust=-2, size=3, colour="black", fontface="bold",angle=360)

# Count By DayOfWeek
e.weekday <- reports %>% group_by(DayOfWeek) %>% summarise(Count=n()) %>% ggplot(aes(x=DayOfWeek, y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Day Of Week", x="DayOfWeek",y="Count") + geom_text(aes(x=DayOfWeek, y=0.01, label= Count), vjust=-2, size=3, colour="black", fontface="bold",angle=360) 

# Count By HourOfDay
e.hour <- reports %>% group_by(HourOfDay) %>% summarise(Count=n()) %>% ggplot(aes(x=fct_rev(HourOfDay), y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Hour Of Day", x="HourOfDay",y="Count") + geom_text(aes(x=HourOfDay, y=0.01, label= Count), vjust=0.5, hjust=-.25, size=3, colour="black", fontface="bold",angle=360) + coord_flip()

# Count By DayOfMonth
e.month <- reports %>% group_by(DayOfMonth) %>% summarise(Count=n()) %>% ggplot(aes(x=fct_rev(DayOfMonth), y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Reports by Day Of Month", x="DayOfMonth",y="Count") + geom_text(aes(x=DayOfMonth, y=0.01, label= Count), vjust=0.5, hjust=-.25, size=3, colour="black", fontface="bold",angle=360) + coord_flip()

# Count By ReportCategory
e.categorySum <- reports %>% group_by(ReportCategory) %>% summarise(Count=n()) %>% ggplot(aes(x=fct_rev(ReportCategory), y=Count)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Sum of Reports By Category", x="Report Category",y="Reports") + geom_text(aes(x=ReportCategory, y=0.01, label= Count), vjust=0.5, hjust=-.25, size=3, colour="black", fontface="bold",angle=360) + coord_flip()

# Avg Delivery time by ReportCategory 
e.categoryDelivery <- reports %>% group_by(ReportCategory) %>% summarise(MeanDelivery=round(mean(ReportDeliveryTime),0)) %>% ggplot(aes(x=fct_rev(ReportCategory), y=MeanDelivery)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Mean Delivery Time By Category", x="Report Category",y="Seconds") + geom_text(aes(x=ReportCategory, y=0.01, label= MeanDelivery), vjust=0.5, hjust=-.25, size=3, colour="black", fontface="bold",angle=360) + coord_flip()

# Avg Build time by ReportCategory 
e.categoryBuild <- reports %>% group_by(ReportCategory) %>% summarise(AvgTime=round(mean(ReportBuildTime),0)) %>% ggplot(aes(x=fct_rev(ReportCategory), y=AvgTime)) + geom_bar(stat="identity", fill="orange", color="grey40") + labs(title="Mean Build Time By Category", x="Report Category",y="Seconds") + geom_text(aes(x=ReportCategory, y=0.01, label= AvgTime), vjust=0.5, hjust=-.25, size=3, colour="black", fontface="bold",angle=360) + coord_flip()

grid.arrange(e.freq, e.priority, e.format, e.delivery,  ncol=2, nrow=2)
grid.arrange(e.server,e.weekday,  ncol=1, nrow=2)

# Buildtime by server
reports %>% ggplot(aes(x=log(ReportBuildTime), fill=Server)) + geom_density(alpha = 0.7) + labs(title="Report Build time colored by Server")   


```


```{r EDA plots2, fig.width=7.25, fig.height=7.25, cache=TRUE}
grid.arrange(e.categorySum, e.categoryDelivery, e.categoryBuild, ncol=1, nrow=3)

grid.arrange(e.hour,e.month,  ncol=2, nrow=1)
```
